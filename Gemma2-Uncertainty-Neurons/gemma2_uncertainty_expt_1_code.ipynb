{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BYFbcODPO7e"
   },
   "source": [
    "# Environment Setup & Model Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wy5ersCMQWFi"
   },
   "source": [
    "## System Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3620,
     "status": "ok",
     "timestamp": 1757717602547,
     "user": {
      "displayName": "Ray",
      "userId": "00599642212095616030"
     },
     "user_tz": 420
    },
    "id": "hW7DOf11PDZG",
    "outputId": "163de8bc-f331-4282-f0ba-bd0d91b41d6a"
   },
   "outputs": [],
   "source": [
    "# 检查GPU和内存状态\n",
    "import torch\n",
    "import psutil\n",
    "\n",
    "print(\"=== 系统信息 ===\")\n",
    "print(f\"Python版本: {torch.__version__}\")\n",
    "print(f\"CUDA可用: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU设备: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU内存: {torch.cuda.get_device_properties(0).total_memory // 1024**3} GB\")\n",
    "\n",
    "print(f\"系统RAM: {psutil.virtual_memory().total // 1024**3} GB\")\n",
    "print(f\"可用RAM: {psutil.virtual_memory().available // 1024**3} GB\")\n",
    "\n",
    "# 设置内存增长策略\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EI62cz-LQZDZ"
   },
   "source": [
    "## Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27183,
     "status": "ok",
     "timestamp": 1757717629735,
     "user": {
      "displayName": "Ray",
      "userId": "00599642212095616030"
     },
     "user_tz": 420
    },
    "id": "snmcaSJqPsFi",
    "outputId": "bcc4c415-218d-4272-e082-6270d6c3b606"
   },
   "outputs": [],
   "source": [
    "# 安装核心库 (移除bitsandbytes，因为不需要量化)\n",
    "!pip install --quiet transformers>=4.40.0\n",
    "!pip install --quiet torch>=2.0.0\n",
    "!pip install --quiet accelerate\n",
    "!pip install --quiet plotly\n",
    "!pip install --quiet numpy pandas matplotlib seaborn\n",
    "!pip install --quiet tqdm\n",
    "\n",
    "# 重启运行时（运行完这个cell后，在菜单栏选择\"运行时\" -> \"重启运行时\"）\n",
    "print(\"安装完成！请重启运行时然后继续下一步。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xv24tiDzSV16"
   },
   "source": [
    "## Restart Reminder\n",
    "⚠️ Important: After installation, please select \"Runtime\" → \"Restart session\" from the menu bar, then continue running the following cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DP7jDdfZSngO"
   },
   "source": [
    "## Load Gemma 2 2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263,
     "referenced_widgets": [
      "af943e5689a94d99b4e14d9fab6fb441",
      "9d1086d5889543f0839b4cd5ad7f4735",
      "2f5248b2c94a41b09f8094344b80f306",
      "51321c3847cd4de1802f420ae6363c33",
      "96d4a9152a4f4611a52895cdbe4045da",
      "f3e0adbd96894b5eab19f097aaf31354",
      "618fd2b8b4f6446b97032145c448810a",
      "690b9b77f7b54bcbbfe246e01ab197de",
      "65d3426321e44edca52c6850f1494c79",
      "ab4253bd986c4a6b948ea0c2d16427d9",
      "4305d4c9e506498982f1358da9766115"
     ]
    },
    "executionInfo": {
     "elapsed": 15476,
     "status": "ok",
     "timestamp": 1757718005362,
     "user": {
      "displayName": "Ray",
      "userId": "00599642212095616030"
     },
     "user_tz": 420
    },
    "id": "YufRwoxtP4kh",
    "outputId": "fba20802-b873-4b01-935d-6d6d75af118e"
   },
   "outputs": [],
   "source": [
    "# Load Gemma 2 2B in FP16\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "print(\"开始加载Gemma 2 2B模型 (FP16精度)...\")\n",
    "\n",
    "# 加载模型和tokenizer\n",
    "try:\n",
    "    print(\"正在加载tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n",
    "\n",
    "    print(\"正在加载模型（FP16精度，这可能需要几分钟）...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"google/gemma-2-2b\",\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,  # 使用FP16精度\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,  # 降低CPU内存使用\n",
    "    )\n",
    "\n",
    "    print(f\"✅ 模型成功加载到设备: {next(model.parameters()).device}\")\n",
    "    print(f\"模型数据类型: {next(model.parameters()).dtype}\")\n",
    "    print(f\"模型参数数量: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "    # 检查模型结构\n",
    "    print(f\"模型层数: {model.config.num_hidden_layers}\")\n",
    "    print(f\"隐藏层维度: {model.config.hidden_size}\")\n",
    "    print(f\"词汇表大小: {model.config.vocab_size}\")\n",
    "\n",
    "    # 显示内存使用情况\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU内存使用: {torch.cuda.memory_allocated() // 1024**2} MB\")\n",
    "        print(f\"GPU内存缓存: {torch.cuda.memory_reserved() // 1024**2} MB\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ 模型加载失败: {e}\")\n",
    "    print(\"请检查网络连接或尝试重新运行此cell\")\n",
    "    print(\"如果内存不足，请考虑使用更小的模型或者启用量化\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z08XaAdHSxzE"
   },
   "source": [
    "## Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1241,
     "status": "ok",
     "timestamp": 1757718008432,
     "user": {
      "displayName": "Ray",
      "userId": "00599642212095616030"
     },
     "user_tz": 420
    },
    "id": "cHZxoJWDQDM_",
    "outputId": "383b4a0d-dffc-4e47-f570-4677d7cb4c00"
   },
   "outputs": [],
   "source": [
    "# 测试模型生成\n",
    "test_prompt = \"The capital of France is\"\n",
    "print(f\"测试提示: '{test_prompt}'\")\n",
    "\n",
    "# 编码输入\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
    "print(f\"输入token数量: {len(inputs['input_ids'][0])}\")\n",
    "print(f\"输入tokens: {tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])}\")\n",
    "\n",
    "# 生成文本\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        max_new_tokens=10,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"生成结果: '{generated_text}'\")\n",
    "\n",
    "# 获取logits分析\n",
    "with torch.no_grad():\n",
    "    model_outputs = model(**inputs, output_hidden_states=True)\n",
    "    logits = model_outputs.logits[0, -1]  # 最后一个token的logits\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    # 获取top-5预测\n",
    "    top_probs, top_indices = torch.topk(probs, 5)\n",
    "    print(\"\\n=== Top 5 预测 ===\")\n",
    "    for i, (prob, idx) in enumerate(zip(top_probs, top_indices)):\n",
    "        token = tokenizer.decode([idx])\n",
    "        print(f\"{i+1}. '{token}' - 概率: {prob:.4f}\")\n",
    "\n",
    "    # 熵计算\n",
    "    def safe_entropy_calculation(logits):\n",
    "        \"\"\"更稳健的熵计算方法\"\"\"\n",
    "        try:\n",
    "            # 检查logits是否异常\n",
    "            if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
    "                print(\"❌ Logits包含异常值\")\n",
    "                return None\n",
    "\n",
    "            # 使用log_softmax避免数值问题\n",
    "            log_probs = torch.log_softmax(logits, dim=-1)\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "            # 检查概率是否异常\n",
    "            if torch.isnan(probs).any() or torch.isinf(probs).any():\n",
    "                print(\"❌ 概率计算异常\")\n",
    "                return None\n",
    "\n",
    "            # 计算熵\n",
    "            entropy = -torch.sum(probs * log_probs)\n",
    "\n",
    "            if torch.isnan(entropy) or torch.isinf(entropy):\n",
    "                print(\"❌ 熵计算结果异常\")\n",
    "                return None\n",
    "\n",
    "            return entropy.item()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 熵计算错误: {e}\")\n",
    "            return None\n",
    "\n",
    "    entropy = safe_entropy_calculation(logits)\n",
    "    if entropy is not None:\n",
    "        print(f\"\\n✅ 预测熵（不确定性）: {entropy:.4f}\")\n",
    "    else:\n",
    "        print(f\"\\n❌ 熵计算失败\")\n",
    "        # 使用替代度量\n",
    "        top1_prob = torch.max(torch.softmax(logits, dim=-1))\n",
    "        print(f\"替代度量 - Top-1概率: {top1_prob:.4f} (越高越确定)\")\n",
    "        print(f\"不确定性估计: {1-top1_prob:.4f}\")\n",
    "\n",
    "print(\"\\n✅ 模型测试完成，准备开始分析不确定性神经元！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upiLI48SFJ49"
   },
   "source": [
    "# Neuron Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRwo-HD9XIZ8"
   },
   "source": [
    "## Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1757718016538,
     "user": {
      "displayName": "Ray",
      "userId": "00599642212095616030"
     },
     "user_tz": 420
    },
    "id": "xeEWwmPQXK_U",
    "outputId": "2402e032-1ac3-4452-ec43-6caf2d23e39c"
   },
   "outputs": [],
   "source": [
    "# 不确定性神经元测试数据集\n",
    "# 基于不同类型的不确定性设计测试句子\n",
    "\n",
    "uncertainty_test_dataset = {\n",
    "\n",
    "    # 1. Epistemic Uncertainty (认知不确定性)\n",
    "    # 模型知识不足导致的不确定性，理论上可以通过更多训练数据解决\n",
    "    \"epistemic\": {\n",
    "        \"description\": \"模型知识不足导致的不确定性\",\n",
    "        \"expected_behavior\": \"高不确定性，模型不知道答案\",\n",
    "        \"sentences\": [\n",
    "            # Obscure factual knowledge\n",
    "            \"The population of Nauru in 2024 is\",\n",
    "            \"The CEO of startup company Zephyr Labs is\",\n",
    "            \"The atomic weight of Flerovium is\",\n",
    "            \"The mayor of Vaduz, Liechtenstein is\",\n",
    "            \"The 47th element on the periodic table is\",\n",
    "            \"The director of the 1927 film Metropolis was\",\n",
    "            \"The winner of the 1952 Nobel Prize in Chemistry was\",\n",
    "            \"The height of Mount Vinson in Antarctica is\",\n",
    "            # Technical/specialized knowledge\n",
    "            \"The Hausdorff dimension of the Sierpinski triangle is\",\n",
    "            \"The IUPAC name for water is\",\n",
    "            \"The half-life of Carbon-14 is\",\n",
    "            \"The speed of sound in helium at 20°C is\"\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # 2. Aleatoric Uncertainty (随机不确定性)\n",
    "    # 输入本身固有的模糊性，即使有完美模型也无法确定\n",
    "    \"aleatoric\": {\n",
    "        \"description\": \"输入固有的模糊性和多义性\",\n",
    "        \"expected_behavior\": \"中等到高不确定性，多个合理答案\",\n",
    "        \"sentences\": [\n",
    "            # Subjective/opinion-based completions\n",
    "            \"The best programming language is\",\n",
    "            \"The most important thing in life is\",\n",
    "            \"The greatest movie of all time is\",\n",
    "            \"The most beautiful color is\",\n",
    "            \"The meaning of happiness is\",\n",
    "            \"Success is defined as\",\n",
    "            # Open-ended continuations with multiple valid paths\n",
    "            \"She told him that\",\n",
    "            \"They decided to go\",\n",
    "            \"The reason for this is\",\n",
    "            \"After thinking about it,\",\n",
    "            \"The story ended when\",\n",
    "            # Lexical ambiguity (word-level multiple meanings)\n",
    "            \"The bank is\",\n",
    "            \"The bat flew\",\n",
    "            \"She couldn't bear\",\n",
    "            \"The seal swam\",\n",
    "            \"The light solution is\"\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # 3. Linguistic Uncertainty (语言不确定性)\n",
    "    # 语言结构或语法导致的不确定性\n",
    "    \"linguistic\": {\n",
    "        \"description\": \"语言结构和语法导致的不确定性\",\n",
    "        \"expected_behavior\": \"结构性不确定性，语法解析困难\",\n",
    "        \"sentences\": [\n",
    "            # PP-attachment ambiguity\n",
    "            \"The man saw the boy with the telescope\",\n",
    "            \"She hit the man with the umbrella\",\n",
    "            \"They discussed the plan in the office\",\n",
    "            \"I saw the Grand Canyon flying to New York\",\n",
    "            # Syntactic ambiguity\n",
    "            \"Flying planes can be dangerous\",\n",
    "            \"They are hunting dogs\",\n",
    "            \"Visiting relatives can be boring\",\n",
    "            \"The shooting of the hunters was terrible\",\n",
    "            # Garden path sentences\n",
    "            \"The horse raced past the barn fell\",\n",
    "            \"The old man the boats\",\n",
    "            \"The complex houses married and single soldiers\",\n",
    "            \"The prime number few\",\n",
    "            # Coordination ambiguity\n",
    "            \"Old men and women were served first\",\n",
    "            \"I saw her duck and cover\"\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # 4. Low Uncertainty Controls (低不确定性对照组)\n",
    "    # 确定性高的句子，作为基线对照\n",
    "    \"low_uncertainty\": {\n",
    "        \"description\": \"高确定性句子，作为基线对照\",\n",
    "        \"expected_behavior\": \"低不确定性，明确的预期答案\",\n",
    "        \"sentences\": [\n",
    "            # Basic facts\n",
    "            \"The capital of USA is\",\n",
    "            \"Two plus two equals\",\n",
    "            \"The sun rises in the\",\n",
    "            \"The first letter of the alphabet is\",\n",
    "            \"Christmas is celebrated on December\",\n",
    "            # Common knowledge\n",
    "            \"The color of grass is\",\n",
    "            \"The Earth orbits the\",\n",
    "            \"The opposite of hot is\",\n",
    "            \"One meter equals one hundred\"\n",
    "        ]\n",
    "    },\n",
    "}\n",
    "\n",
    "def print_dataset_summary():\n",
    "    \"\"\"打印数据集摘要\"\"\"\n",
    "    total_sentences = 0\n",
    "    print(\"=== 不确定性测试数据集摘要 ===\\n\")\n",
    "\n",
    "    for category, data in uncertainty_test_dataset.items():\n",
    "        num_sentences = len(data[\"sentences\"])\n",
    "        total_sentences += num_sentences\n",
    "\n",
    "        print(f\"📊 {category.upper().replace('_', ' ')} ({num_sentences} 句子)\")\n",
    "        print(f\"   描述: {data['description']}\")\n",
    "        print(f\"   预期行为: {data['expected_behavior']}\")\n",
    "        print(\"   示例句子:\")\n",
    "        for i, sentence in enumerate(data[\"sentences\"][:2]):  # 只显示前2个\n",
    "            print(f\"     • '{sentence}'\")\n",
    "        if num_sentences > 2:\n",
    "            print(f\"     ... 还有 {num_sentences - 2} 个句子\")\n",
    "        print()\n",
    "\n",
    "    print(f\"📈 总计: {total_sentences} 个测试句子\")\n",
    "    print(f\"⏱️  预计实验时间: {total_sentences * 0.5:.1f}-{total_sentences * 1:.1f} 分钟\")\n",
    "    return total_sentences\n",
    "\n",
    "def get_test_sentences_by_category(category=None, limit_per_category=None):\n",
    "    \"\"\"\n",
    "    获取指定类别的测试句子\n",
    "\n",
    "    Args:\n",
    "        category: 指定类别，None表示全部\n",
    "        limit_per_category: 每个类别的句子数量限制\n",
    "    \"\"\"\n",
    "    if category and category in uncertainty_test_dataset:\n",
    "        sentences = uncertainty_test_dataset[category][\"sentences\"]\n",
    "        if limit_per_category:\n",
    "            sentences = sentences[:limit_per_category]\n",
    "        return [(sentence, category) for sentence in sentences]\n",
    "\n",
    "    # 返回所有类别\n",
    "    all_sentences = []\n",
    "    for cat, data in uncertainty_test_dataset.items():\n",
    "        sentences = data[\"sentences\"]\n",
    "        if limit_per_category:\n",
    "            sentences = sentences[:limit_per_category]\n",
    "        all_sentences.extend([(sentence, cat) for sentence in sentences])\n",
    "\n",
    "    return all_sentences\n",
    "\n",
    "def get_recommended_test_set(quick_test=True):\n",
    "    \"\"\"\n",
    "    获取推荐的测试集\n",
    "\n",
    "    Args:\n",
    "        quick_test: True = 快速测试(每类3-4句), False = 完整测试\n",
    "    \"\"\"\n",
    "    if quick_test:\n",
    "        print(\"🚀 推荐：快速测试集 (每类3-4个句子，总计约15个)\")\n",
    "        limit = 4\n",
    "    else:\n",
    "        print(\"🔬 推荐：完整测试集 (所有句子)\")\n",
    "        limit = None\n",
    "\n",
    "    return get_test_sentences_by_category(limit_per_category=limit)\n",
    "\n",
    "# 显示数据集信息\n",
    "total_count = print_dataset_summary()\n",
    "\n",
    "# 提供使用建议\n",
    "print(\"💡 使用建议:\")\n",
    "print(\"   • 第一次实验：使用 get_recommended_test_set(quick_test=True)\")\n",
    "print(\"   • 详细分析：使用 get_recommended_test_set(quick_test=False)\")\n",
    "print(\"   • 特定分析：使用 get_test_sentences_by_category('epistemic')\")\n",
    "print(\"\\n✅ 测试数据集准备完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnOSDG5kFmSJ"
   },
   "source": [
    "## Weight Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 867,
     "status": "ok",
     "timestamp": 1757718030243,
     "user": {
      "displayName": "Ray",
      "userId": "00599642212095616030"
     },
     "user_tz": 420
    },
    "id": "dbwQIjYPXX3R",
    "outputId": "03f50f6d-8308-4bb0-c055-540a08eecfa9"
   },
   "outputs": [],
   "source": [
    "print(\"开始提取最后一层神经元权重...\")\n",
    "\n",
    "# 获取模型的最后一层\n",
    "last_layer_idx = model.config.num_hidden_layers - 1\n",
    "print(f\"分析第{last_layer_idx}层（最后一层）\")\n",
    "\n",
    "# 提取最后一层的输出权重和unembedding矩阵\n",
    "try:\n",
    "    # Gemma 2的结构访问\n",
    "    last_layer = model.model.layers[last_layer_idx]\n",
    "\n",
    "    # 获取MLP的输出权重 (hidden_size, intermediate_size)\n",
    "    mlp_gate_proj = last_layer.mlp.gate_proj.weight.data  # (intermediate_size, hidden_size)\n",
    "    mlp_up_proj = last_layer.mlp.up_proj.weight.data      # (intermediate_size, hidden_size)\n",
    "    mlp_down_proj = last_layer.mlp.down_proj.weight.data  # (hidden_size, intermediate_size)\n",
    "\n",
    "    # 获取unembedding矩阵\n",
    "    unembed_matrix = model.lm_head.weight.data  # (vocab_size, hidden_size)\n",
    "\n",
    "    print(f\"MLP gate projection形状: {mlp_gate_proj.shape}\")\n",
    "    print(f\"MLP up projection形状: {mlp_up_proj.shape}\")\n",
    "    print(f\"MLP down projection形状: {mlp_down_proj.shape}\")\n",
    "    print(f\"Unembedding矩阵形状: {unembed_matrix.shape}\")\n",
    "\n",
    "    # 计算有效的输出权重 (我们关注down_proj，它是MLP的输出)\n",
    "    W_out = mlp_down_proj.T  # 转置为 (intermediate_size, hidden_size)\n",
    "    print(f\"输出权重矩阵形状: {W_out.shape}\")\n",
    "    print(f\"设备: {W_out.device}\")\n",
    "    print(f\"数据类型: {W_out.dtype}\")\n",
    "\n",
    "    # 转换为CPU进行分析，保持FP16精度\n",
    "    W_out_cpu = W_out.cpu()  # 保持FP16\n",
    "    unembed_cpu = unembed_matrix.cpu()  # 保持FP16\n",
    "\n",
    "    print(\"✅ 权重提取完成！\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ 权重提取失败: {e}\")\n",
    "    print(\"模型结构可能与预期不同，让我们检查实际结构...\")\n",
    "\n",
    "    # 打印模型结构以便调试\n",
    "    print(\"\\n=== 模型结构检查 ===\")\n",
    "    for name, module in model.named_modules():\n",
    "        if 'layer' in name and 'mlp' in name:\n",
    "            print(f\"{name}: {type(module)}\")\n",
    "            if hasattr(module, 'weight'):\n",
    "                print(f\"  权重形状: {module.weight.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfzgwtF2FnZ1"
   },
   "source": [
    "## LogitVar Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26336,
     "status": "ok",
     "timestamp": 1757719108689,
     "user": {
      "displayName": "Ray",
      "userId": "00599642212095616030"
     },
     "user_tz": 420
    },
    "id": "gHV5gOzuXawq",
    "outputId": "e98ffe42-1313-495d-a634-e1827b603215"
   },
   "outputs": [],
   "source": [
    "# 计算每个神经元的LogitVar和权重范数\n",
    "print(\"计算LogitVar指标...\")\n",
    "\n",
    "def calculate_logit_var(neuron_weights, unembed_matrix):\n",
    "    \"\"\"\n",
    "    计算神经元的LogitVar指标\n",
    "    LogitVar(i) = Var(W_out^(i) @ W_U / ||W_out^(i) @ W_U||_2)\n",
    "    \"\"\"\n",
    "    # 确保计算精度，转换为float32进行数值计算\n",
    "    neuron_weights_f32 = neuron_weights.float()\n",
    "    unembed_matrix_f32 = unembed_matrix.float()\n",
    "\n",
    "    # 计算神经元权重与unembedding的乘积\n",
    "    projections = neuron_weights_f32 @ unembed_matrix_f32.T\n",
    "    norms = torch.norm(projections, dim=1, keepdim=True)\n",
    "\n",
    "    # 计算每个神经元投影的L2范数 标准化投影\n",
    "    norms = torch.clamp(norms, min=1e-10)  # 增加最小值\n",
    "    normalized_projections = projections / norms\n",
    "\n",
    "    # 计算每个神经元标准化投影的方差\n",
    "    mask = torch.isfinite(normalized_projections).all(dim=1)\n",
    "    logit_vars = torch.full((normalized_projections.shape[0],), float('nan'))\n",
    "    logit_vars[mask] = torch.var(normalized_projections[mask], dim=1)\n",
    "\n",
    "    return logit_vars, norms.squeeze()\n",
    "\n",
    "# 执行计算\n",
    "try:\n",
    "    print(f\"计算{W_out_cpu.shape[0]}个神经元的指标...\")\n",
    "\n",
    "    logit_vars, projection_norms = calculate_logit_var(W_out_cpu, unembed_cpu)\n",
    "\n",
    "    # 计算输出权重的L2范数\n",
    "    weight_norms = torch.norm(W_out_cpu.float(), dim=1)  # 每个神经元权重向量的范数\n",
    "\n",
    "    print(f\"LogitVar计算完成: {logit_vars.shape}\")\n",
    "    print(f\"权重范数计算完成: {weight_norms.shape}\")\n",
    "    print(f\"投影范数计算完成: {projection_norms.shape}\")\n",
    "\n",
    "    # 基本统计\n",
    "    print(f\"\\n=== 统计摘要 ===\")\n",
    "    print(f\"LogitVar - 均值: {logit_vars.mean():.6f}, 标准差: {logit_vars.std():.6f}\")\n",
    "    print(f\"权重范数 - 均值: {weight_norms.mean():.6f}, 标准差: {weight_norms.std():.6f}\")\n",
    "    print(f\"投影范数 - 均值: {projection_norms.mean():.6f}, 标准差: {projection_norms.std():.6f}\")\n",
    "\n",
    "    # 寻找异常值（低LogitVar但高权重范数的神经元）\n",
    "    logit_var_threshold = logit_vars.quantile(0.1)  # 最低10%的LogitVar\n",
    "    weight_norm_threshold = weight_norms.quantile(0.9)  # 最高10%的权重范数\n",
    "\n",
    "    # 候选不确定性神经元\n",
    "    uncertainty_candidates = (logit_vars < logit_var_threshold) & (weight_norms > weight_norm_threshold)\n",
    "    num_candidates = uncertainty_candidates.sum().item()\n",
    "\n",
    "    print(f\"\\n=== 候选不确定性神经元 ===\")\n",
    "    print(f\"低LogitVar阈值: {logit_var_threshold:.6f}\")\n",
    "    print(f\"高权重范数阈值: {weight_norm_threshold:.6f}\")\n",
    "    print(f\"找到候选神经元: {num_candidates} 个\")\n",
    "\n",
    "    if num_candidates > 0:\n",
    "        candidate_indices = torch.where(uncertainty_candidates)[0]\n",
    "        print(f\"候选神经元索引: {candidate_indices.tolist()}\")\n",
    "\n",
    "        # 显示前5个候选神经元的详细信息\n",
    "        for i, idx in enumerate(candidate_indices[:5]):\n",
    "            print(f\"  神经元 {idx.item()}: LogitVar={logit_vars[idx]:.6f}, 权重范数={weight_norms[idx]:.6f}\")\n",
    "\n",
    "    print(\"\\n✅ LogitVar分析完成！\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ 计算失败: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybNIjtr3Fpsg"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1273,
     "status": "ok",
     "timestamp": 1757723969627,
     "user": {
      "displayName": "Ray",
      "userId": "00599642212095616030"
     },
     "user_tz": 420
    },
    "id": "cdk0IYL0FrfT",
    "outputId": "aa18f445-126f-42bf-db6b-a921f4fc7280"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create scatter plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Gemma 2 2B (FP16) Last Layer Neuron Analysis: Finding Uncertainty Neurons', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Main scatter plot: Weight norm vs LogitVar\n",
    "ax1 = axes[0, 0]\n",
    "scatter = ax1.scatter(weight_norms, logit_vars, alpha=0.6, s=30, c='blue', edgecolors='none')\n",
    "ax1.set_xlabel('Weight L2 Norm', fontsize=12)\n",
    "ax1.set_ylabel('LogitVar', fontsize=12)\n",
    "ax1.set_title('Weight Norm vs LogitVar\\n(Bottom-right = Uncertainty Neuron Candidates)', fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Mark candidate uncertainty neurons\n",
    "if num_candidates > 0:\n",
    "    candidate_x = weight_norms[uncertainty_candidates]\n",
    "    candidate_y = logit_vars[uncertainty_candidates]\n",
    "    ax1.scatter(candidate_x, candidate_y, c='red', s=80, marker='o',\n",
    "               edgecolors='black', linewidths=2, alpha=0.8, label=f'Candidates ({num_candidates} neurons)')\n",
    "    ax1.legend()\n",
    "\n",
    "# 2. LogitVar distribution histogram\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(logit_vars.numpy(), bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "ax2.axvline(logit_var_threshold, color='red', linestyle='--', linewidth=2, label=f'10th percentile: {logit_var_threshold:.4f}')\n",
    "ax2.set_xlabel('LogitVar', fontsize=12)\n",
    "ax2.set_ylabel('Number of Neurons', fontsize=12)\n",
    "ax2.set_title('LogitVar Distribution', fontsize=11)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Weight norm distribution histogram\n",
    "ax3 = axes[1, 0]\n",
    "ax3.hist(weight_norms.numpy(), bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
    "ax3.axvline(weight_norm_threshold, color='red', linestyle='--', linewidth=2, label=f'90th percentile: {weight_norm_threshold:.4f}')\n",
    "ax3.set_xlabel('Weight L2 Norm', fontsize=12)\n",
    "ax3.set_ylabel('Number of Neurons', fontsize=12)\n",
    "ax3.set_title('Weight Norm Distribution', fontsize=11)\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Projection norm vs LogitVar\n",
    "ax4 = axes[1, 1]\n",
    "ax4.scatter(projection_norms, logit_vars, alpha=0.6, s=30, c='purple', edgecolors='none')\n",
    "ax4.set_xlabel('Projection Norm (||W_out @ W_U||)', fontsize=12)\n",
    "ax4.set_ylabel('LogitVar', fontsize=12)\n",
    "ax4.set_title('Projection Norm vs LogitVar', fontsize=11)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "if num_candidates > 0:\n",
    "    candidate_proj = projection_norms[uncertainty_candidates]\n",
    "    ax4.scatter(candidate_proj, candidate_y, c='red', s=80, marker='o',\n",
    "               edgecolors='black', linewidths=2, alpha=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed analysis results\n",
    "print(\"=== Detailed Analysis Results ===\")\n",
    "print(f\"Total number of neurons: {len(weight_norms)}\")\n",
    "print(f\"Candidate uncertainty neurons: {num_candidates}\")\n",
    "\n",
    "if num_candidates > 0:\n",
    "    print(f\"\\n=== Top {min(30, num_candidates)} Candidate Neuron Details ===\")\n",
    "    candidate_indices = torch.where(uncertainty_candidates)[0]\n",
    "    for i, idx in enumerate(candidate_indices[:min(30, num_candidates)]):\n",
    "        idx_val = idx.item()\n",
    "        print(f\"Neuron {idx_val:4d}: \"\n",
    "              f\"LogitVar={logit_vars[idx]:.4e}, \"\n",
    "              f\"Weight norm={weight_norms[idx]:.4f}, \"\n",
    "              f\"Projection norm={projection_norms[idx]:.4f}\")\n",
    "\n",
    "    # Save candidate neuron indices for subsequent analysis\n",
    "    top_candidates = candidate_indices[:5] if num_candidates >= 5 else candidate_indices\n",
    "    print(f\"\\nSelecting top {len(top_candidates)} neurons for further validation: {top_candidates.tolist()}\")\n",
    "else:\n",
    "    print(\"No obvious candidate uncertainty neurons found\")\n",
    "    # Select some boundary cases for analysis\n",
    "    sorted_indices = torch.argsort(logit_vars)\n",
    "    top_candidates = sorted_indices[:3]  # Top 3 neurons with lowest LogitVar\n",
    "    print(f\"Selecting 3 neurons with lowest LogitVar for analysis: {top_candidates.tolist()}\")\n",
    "\n",
    "print(\"\\n✅ Visualization analysis completed! Next we will validate the causal effects of these candidate neurons.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWxOF4CpFTm0"
   },
   "source": [
    "# Causal Inference Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXxad-ZMGGuB"
   },
   "source": [
    "## Causal Verification Experiment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1757722301515,
     "user": {
      "displayName": "Ray",
      "userId": "00599642212095616030"
     },
     "user_tz": 420
    },
    "id": "3kGAKxJyYzkM",
    "outputId": "9647d051-7a1c-4a91-9fa3-737f41af9526"
   },
   "outputs": [],
   "source": [
    "# 因果验证：激活补丁实验\n",
    "print(\"准备因果验证实验...\")\n",
    "\n",
    "def calculate_entropy(logits):\n",
    "    \"\"\"计算预测的熵（不确定性度量）\"\"\"\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    entropy = -torch.sum(probs * log_probs, dim=-1)\n",
    "    return entropy\n",
    "\n",
    "def create_hook_fn(layer_idx, neuron_indices, intervention_type='zero'):\n",
    "    \"\"\"创建激活干预的hook函数\"\"\"\n",
    "    def hook_fn(module, input, output):\n",
    "        # Gemma 2的MLP输出通常是hidden states\n",
    "        if intervention_type == 'zero':\n",
    "            # 将指定神经元的激活设为0\n",
    "            for neuron_idx in neuron_indices:\n",
    "                if neuron_idx < output.shape[-1]:\n",
    "                    output[:, :, neuron_idx] = 0\n",
    "        elif intervention_type == 'mean':\n",
    "            # 将指定神经元的激活设为该层的均值\n",
    "            layer_mean = output.mean(dim=[0, 1], keepdim=True)\n",
    "            for neuron_idx in neuron_indices:\n",
    "                if neuron_idx < output.shape[-1]:\n",
    "                    output[:, :, neuron_idx] = layer_mean[:, :, neuron_idx]\n",
    "        return output\n",
    "    return hook_fn\n",
    "\n",
    "# 使用结构化的测试数据集\n",
    "print(\"=== 选择测试句子 ===\")\n",
    "\n",
    "# 使用更大测试集\n",
    "test_data = get_recommended_test_set(quick_test=False)\n",
    "test_sentences = [item[0] for item in test_data]  # 提取句子\n",
    "sentence_categories = [item[1] for item in test_data]  # 提取类别\n",
    "\n",
    "print(f\"选择了 {len(test_sentences)} 个测试句子，涵盖 {len(set(sentence_categories))} 种不确定性类型\")\n",
    "print(\"\\n按类别显示测试句子:\")\n",
    "\n",
    "# 按类别组织显示\n",
    "for category in set(sentence_categories):\n",
    "    category_sentences = [sent for sent, cat in test_data if cat == category]\n",
    "    print(f\"\\n📊 {category.upper().replace('_', ' ')} ({len(category_sentences)} 句子):\")\n",
    "    for i, sentence in enumerate(category_sentences):\n",
    "        print(f\"   {i+1}. '{sentence}'\")\n",
    "\n",
    "# 选择要测试的神经元\n",
    "if 'top_candidates' in locals() and len(top_candidates) > 0:\n",
    "    test_neurons = top_candidates[:5].tolist()  # 测试前5个候选神经元\n",
    "    print(f\"\\n将测试神经元: {test_neurons}\")\n",
    "else:\n",
    "    # 如果没有明显候选，随机选择一些神经元作为对照\n",
    "    test_neurons = [100, 200, 500]  # 示例神经元索引\n",
    "    print(f\"\\n使用示例神经元进行测试: {test_neurons}\")\n",
    "\n",
    "print(\"\\n✅ 验证实验设置完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQvheHSNGKRJ"
   },
   "source": [
    "## Activation Patching Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1757722308768,
     "user": {
      "displayName": "Ray",
      "userId": "00599642212095616030"
     },
     "user_tz": 420
    },
    "id": "7bpaMA7gY2nQ",
    "outputId": "8a0e787b-4297-4331-9ba6-d8a90e864efb"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from contextlib import contextmanager\n",
    "\n",
    "print(\"开始执行激活补丁实验...\")\n",
    "\n",
    "def run_intervention_experiment(model, tokenizer, test_data, neuron_indices, layer_idx):\n",
    "    \"\"\"\n",
    "    运行神经元干预实验\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'sentences': [],\n",
    "        'categories': [],\n",
    "        'baseline_entropy': [],\n",
    "        'zero_entropy': [],\n",
    "        'mean_entropy': [],\n",
    "        'entropy_change_zero': [],\n",
    "        'entropy_change_mean': [],\n",
    "        'baseline_top_tokens': [],\n",
    "        'zero_top_tokens': [],\n",
    "        'mean_top_tokens': []\n",
    "    }\n",
    "\n",
    "    # 获取要干预的层\n",
    "    target_layer = model.model.layers[layer_idx].mlp\n",
    "\n",
    "    for sentence, category in tqdm(test_data, desc=\"测试句子\"):\n",
    "        # 编码输入\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # 1. 基线预测（无干预）\n",
    "        with torch.no_grad():\n",
    "            baseline_outputs = model(**inputs)\n",
    "            baseline_logits = baseline_outputs.logits[0, -1]  # 最后一个token的logits\n",
    "            baseline_entropy = calculate_entropy(baseline_logits).item()\n",
    "\n",
    "            # 获取top-3预测\n",
    "            baseline_probs = torch.softmax(baseline_logits, dim=-1)\n",
    "            baseline_top_probs, baseline_top_indices = torch.topk(baseline_probs, 3)\n",
    "            baseline_top_tokens = [tokenizer.decode([idx]).strip() for idx in baseline_top_indices]\n",
    "\n",
    "        # 2. 零化干预\n",
    "        zero_hook = target_layer.register_forward_hook(\n",
    "            create_hook_fn(layer_idx, neuron_indices, 'zero')\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            zero_outputs = model(**inputs)\n",
    "            zero_logits = zero_outputs.logits[0, -1]\n",
    "            zero_entropy = calculate_entropy(zero_logits).item()\n",
    "\n",
    "            zero_probs = torch.softmax(zero_logits, dim=-1)\n",
    "            zero_top_probs, zero_top_indices = torch.topk(zero_probs, 3)\n",
    "            zero_top_tokens = [tokenizer.decode([idx]).strip() for idx in zero_top_indices]\n",
    "\n",
    "        zero_hook.remove()\n",
    "\n",
    "        # 3. 均值干预\n",
    "        mean_hook = target_layer.register_forward_hook(\n",
    "            create_hook_fn(layer_idx, neuron_indices, 'mean')\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mean_outputs = model(**inputs)\n",
    "            mean_logits = mean_outputs.logits[0, -1]\n",
    "            mean_entropy = calculate_entropy(mean_logits).item()\n",
    "\n",
    "            mean_probs = torch.softmax(mean_logits, dim=-1)\n",
    "            mean_top_probs, mean_top_indices = torch.topk(mean_probs, 3)\n",
    "            mean_top_tokens = [tokenizer.decode([idx]).strip() for idx in mean_top_indices]\n",
    "\n",
    "        mean_hook.remove()\n",
    "\n",
    "        # 计算熵变化\n",
    "        entropy_change_zero = zero_entropy - baseline_entropy\n",
    "        entropy_change_mean = mean_entropy - baseline_entropy\n",
    "\n",
    "        # 保存结果\n",
    "        results['sentences'].append(sentence)\n",
    "        results['categories'].append(category)\n",
    "        results['baseline_entropy'].append(baseline_entropy)\n",
    "        results['zero_entropy'].append(zero_entropy)\n",
    "        results['mean_entropy'].append(mean_entropy)\n",
    "        results['entropy_change_zero'].append(entropy_change_zero)\n",
    "        results['entropy_change_mean'].append(entropy_change_mean)\n",
    "        results['baseline_top_tokens'].append(baseline_top_tokens)\n",
    "        results['zero_top_tokens'].append(zero_top_tokens)\n",
    "        results['mean_top_tokens'].append(mean_top_tokens)\n",
    "\n",
    "        # 实时显示结果\n",
    "        print(f\"\\n--- [{category.upper()}] '{sentence}' ---\")\n",
    "        print(f\"基线熵: {baseline_entropy:.4f}\")\n",
    "        print(f\"零化熵: {zero_entropy:.4f} (Δ: {entropy_change_zero:+.4f})\")\n",
    "        print(f\"均值熵: {mean_entropy:.4f} (Δ: {entropy_change_mean:+.4f})\")\n",
    "        print(f\"Top-3预测: {' | '.join(baseline_top_tokens)}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F94ppKGQGRQM"
   },
   "source": [
    "## Result Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 7654,
     "status": "ok",
     "timestamp": 1757722324063,
     "user": {
      "displayName": "Ray",
      "userId": "00599642212095616030"
     },
     "user_tz": 420
    },
    "id": "LFekZGZAY48j",
    "outputId": "6bfac958-0482-4099-8d4d-83c00a078dde"
   },
   "outputs": [],
   "source": [
    "# Execute complete uncertainty neuron validation experiment\n",
    "print(\"🧪 Starting uncertainty neuron validation experiment\\n\")\n",
    "\n",
    "# Set experiment parameters\n",
    "LAYER_IDX = last_layer_idx  # Use the last layer\n",
    "print(f\"Target layer: Layer {LAYER_IDX}\")\n",
    "print(f\"Test neurons: {test_neurons}\")\n",
    "print(f\"Number of test sentences: {len(test_sentences)}\")\n",
    "\n",
    "# Execute experiment\n",
    "experiment_results = run_intervention_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    test_data=test_data,\n",
    "    neuron_indices=test_neurons,\n",
    "    layer_idx=LAYER_IDX\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Experiment completed! Tested {len(experiment_results['sentences'])} sentences\")\n",
    "\n",
    "# === Results Analysis ===\n",
    "def analyze_results_by_category(results):\n",
    "    \"\"\"Analyze experiment results by uncertainty type\"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    # Convert to DataFrame for easier analysis\n",
    "    df = pd.DataFrame({\n",
    "        'sentence': results['sentences'],\n",
    "        'category': results['categories'],\n",
    "        'baseline_entropy': results['baseline_entropy'],\n",
    "        'entropy_change_zero': results['entropy_change_zero'],\n",
    "        'entropy_change_mean': results['entropy_change_mean']\n",
    "    })\n",
    "\n",
    "    print(\"=== Results Analysis by Uncertainty Type ===\\n\")\n",
    "\n",
    "    # Statistics by category\n",
    "    category_stats = df.groupby('category').agg({\n",
    "        'baseline_entropy': ['mean', 'std', 'count'],\n",
    "        'entropy_change_zero': ['mean', 'std'],\n",
    "        'entropy_change_mean': ['mean', 'std']\n",
    "    }).round(4)\n",
    "\n",
    "    for category in df['category'].unique():\n",
    "        cat_data = df[df['category'] == category]\n",
    "        n_samples = len(cat_data)\n",
    "\n",
    "        print(f\"📊 {category.upper().replace('_', ' ')} ({n_samples} samples)\")\n",
    "        print(f\"   Baseline entropy: {cat_data['baseline_entropy'].mean():.4f} ± {cat_data['baseline_entropy'].std():.4f}\")\n",
    "        print(f\"   Zero intervention effect: {cat_data['entropy_change_zero'].mean():.4f} ± {cat_data['entropy_change_zero'].std():.4f}\")\n",
    "        print(f\"   Mean intervention effect: {cat_data['entropy_change_mean'].mean():.4f} ± {cat_data['entropy_change_mean'].std():.4f}\")\n",
    "\n",
    "        # Effect direction analysis\n",
    "        zero_positive = (cat_data['entropy_change_zero'] > 0).sum()\n",
    "        mean_positive = (cat_data['entropy_change_mean'] > 0).sum()\n",
    "        print(f\"   Proportion with zero intervention increasing entropy: {zero_positive/n_samples:.1%}\")\n",
    "        print(f\"   Proportion with mean intervention increasing entropy: {mean_positive/n_samples:.1%}\")\n",
    "        print()\n",
    "\n",
    "    return df\n",
    "\n",
    "# Execute analysis\n",
    "results_df = analyze_results_by_category(experiment_results)\n",
    "\n",
    "# === Visualize Analysis Results ===\n",
    "def plot_experiment_results(df):\n",
    "    \"\"\"Visualize experiment results\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Uncertainty Neuron Intervention Experiment Results Analysis (FP16)', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # 1. Baseline entropy distribution by category\n",
    "    ax1 = axes[0, 0]\n",
    "    categories = df['category'].unique()\n",
    "    colors = sns.color_palette(\"husl\", len(categories))\n",
    "\n",
    "    for i, category in enumerate(categories):\n",
    "        cat_data = df[df['category'] == category]\n",
    "        ax1.scatter(cat_data.index, cat_data['baseline_entropy'],\n",
    "                   label=category.replace('_', ' '), alpha=0.7, s=60, color=colors[i])\n",
    "\n",
    "    ax1.set_xlabel('Sample Index')\n",
    "    ax1.set_ylabel('Baseline Entropy')\n",
    "    ax1.set_title('Baseline Uncertainty for Different Sentence Types')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Intervention effect comparison\n",
    "    ax2 = axes[0, 1]\n",
    "    category_means = df.groupby('category')[['entropy_change_zero', 'entropy_change_mean']].mean()\n",
    "    x = np.arange(len(categories))\n",
    "    width = 0.35\n",
    "\n",
    "    ax2.bar(x - width/2, category_means['entropy_change_zero'], width,\n",
    "           label='Zero Intervention', alpha=0.8, color='red')\n",
    "    ax2.bar(x + width/2, category_means['entropy_change_mean'], width,\n",
    "           label='Mean Intervention', alpha=0.8, color='blue')\n",
    "\n",
    "    ax2.set_xlabel('Uncertainty Type')\n",
    "    ax2.set_ylabel('Average Entropy Change')\n",
    "    ax2.set_title('Comparison of Different Intervention Methods')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels([cat.replace('_', '\\n') for cat in categories])\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "\n",
    "    # 3. Scatter plot: baseline entropy vs intervention effect\n",
    "    ax3 = axes[1, 0]\n",
    "    for i, category in enumerate(categories):\n",
    "        cat_data = df[df['category'] == category]\n",
    "        ax3.scatter(cat_data['baseline_entropy'], cat_data['entropy_change_zero'],\n",
    "                   label=category.replace('_', ' '), alpha=0.7, s=60, color=colors[i])\n",
    "\n",
    "    ax3.set_xlabel('Baseline Entropy')\n",
    "    ax3.set_ylabel('Zero Intervention Entropy Change')\n",
    "    ax3.set_title('Baseline Uncertainty vs Intervention Effect')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "\n",
    "    # 4. Effect strength distribution\n",
    "    ax4 = axes[1, 1]\n",
    "    all_changes = np.concatenate([df['entropy_change_zero'], df['entropy_change_mean']])\n",
    "    intervention_types = ['Zero Intervention'] * len(df) + ['Mean Intervention'] * len(df)\n",
    "\n",
    "    ax4.hist([df['entropy_change_zero'], df['entropy_change_mean']],\n",
    "            bins=10, alpha=0.7, label=['Zero Intervention', 'Mean Intervention'], color=['red', 'blue'])\n",
    "\n",
    "    ax4.set_xlabel('Entropy Change')\n",
    "    ax4.set_ylabel('Frequency')\n",
    "    ax4.set_title('Distribution of Intervention Effect Strength')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.axvline(x=0, color='black', linestyle='-', alpha=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate analysis charts\n",
    "plot_experiment_results(results_df)\n",
    "\n",
    "# === Experiment Conclusions ===\n",
    "print(\"=== 🎯 Experiment Conclusions ===\")\n",
    "\n",
    "# Calculate overall effects\n",
    "overall_zero_effect = results_df['entropy_change_zero'].mean()\n",
    "overall_mean_effect = results_df['entropy_change_mean'].mean()\n",
    "\n",
    "print(f\"Overall intervention effects:\")\n",
    "print(f\"  Zero intervention average effect: {overall_zero_effect:.4f}\")\n",
    "print(f\"  Mean intervention average effect: {overall_mean_effect:.4f}\")\n",
    "\n",
    "# Statistics for significant effects\n",
    "significant_zero = (abs(results_df['entropy_change_zero']) > 0.1).sum()\n",
    "significant_mean = (abs(results_df['entropy_change_mean']) > 0.1).sum()\n",
    "total_samples = len(results_df)\n",
    "\n",
    "print(f\"\\nSignificant effect statistics (|change| > 0.1):\")\n",
    "print(f\"  Zero intervention significant effects: {significant_zero}/{total_samples} ({significant_zero/total_samples:.1%})\")\n",
    "print(f\"  Mean intervention significant effects: {significant_mean}/{total_samples} ({significant_mean/total_samples:.1%})\")\n",
    "\n",
    "# Uncertainty neuron determination\n",
    "if overall_zero_effect > 0.05 or overall_mean_effect > 0.05:\n",
    "    print(f\"\\n✅ Conclusion: Neurons {test_neurons} may be uncertainty neurons!\")\n",
    "    print(\"   - Intervention on these neurons significantly affected model prediction uncertainty\")\n",
    "    print(\"   - Recommend conducting deeper analysis and testing more neurons\")\n",
    "else:\n",
    "    print(f\"\\n❓ Conclusion: The uncertainty role of neurons {test_neurons} is not obvious\")\n",
    "    print(\"   - Recommend testing other candidate neurons\")\n",
    "    print(\"   - Or try different intervention methods\")\n",
    "\n",
    "print(f\"\\n🎉 Your first mechanistic interpretability experiment is complete!\")\n",
    "print(\"   Next steps you can try:\")\n",
    "print(\"   • Test neurons from more layers\")\n",
    "print(\"   • Use a larger test dataset\")\n",
    "print(\"   • Implement more refined activation patching methods\")\n",
    "print(\"   • Analyze specific types of uncertainty neurons\")\n",
    "\n",
    "# Display current memory usage\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nCurrent GPU memory usage: {torch.cuda.memory_allocated() // 1024**2} MB\")\n",
    "    print(f\"Peak GPU memory: {torch.cuda.max_memory_allocated() // 1024**2} MB\")\n",
    "    print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory // 1024**3} GB\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOyt51BexA8L38qD5skep9m",
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2f5248b2c94a41b09f8094344b80f306": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_690b9b77f7b54bcbbfe246e01ab197de",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_65d3426321e44edca52c6850f1494c79",
      "value": 3
     }
    },
    "4305d4c9e506498982f1358da9766115": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "51321c3847cd4de1802f420ae6363c33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ab4253bd986c4a6b948ea0c2d16427d9",
      "placeholder": "​",
      "style": "IPY_MODEL_4305d4c9e506498982f1358da9766115",
      "value": " 3/3 [00:03&lt;00:00,  1.06it/s]"
     }
    },
    "618fd2b8b4f6446b97032145c448810a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "65d3426321e44edca52c6850f1494c79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "690b9b77f7b54bcbbfe246e01ab197de": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "96d4a9152a4f4611a52895cdbe4045da": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9d1086d5889543f0839b4cd5ad7f4735": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f3e0adbd96894b5eab19f097aaf31354",
      "placeholder": "​",
      "style": "IPY_MODEL_618fd2b8b4f6446b97032145c448810a",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "ab4253bd986c4a6b948ea0c2d16427d9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af943e5689a94d99b4e14d9fab6fb441": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9d1086d5889543f0839b4cd5ad7f4735",
       "IPY_MODEL_2f5248b2c94a41b09f8094344b80f306",
       "IPY_MODEL_51321c3847cd4de1802f420ae6363c33"
      ],
      "layout": "IPY_MODEL_96d4a9152a4f4611a52895cdbe4045da"
     }
    },
    "f3e0adbd96894b5eab19f097aaf31354": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
